{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ML Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Dates</th><th>Category</th><th>Descript</th><th>DayOfWeek</th><th>PdDistrict</th><th>Resolution</th><th>Address</th><th>X</th><th>Y</th></tr></thead><tbody><tr><td>2015-05-13T23:53:00.000+0000</td><td>WARRANTS</td><td>WARRANT ARREST</td><td>Wednesday</td><td>NORTHERN</td><td>ARREST, BOOKED</td><td>OAK ST / LAGUNA ST</td><td>-122.425891675136</td><td>37.7745985956747</td></tr><tr><td>2015-05-13T23:53:00.000+0000</td><td>OTHER OFFENSES</td><td>TRAFFIC VIOLATION ARREST</td><td>Wednesday</td><td>NORTHERN</td><td>ARREST, BOOKED</td><td>OAK ST / LAGUNA ST</td><td>-122.425891675136</td><td>37.7745985956747</td></tr><tr><td>2015-05-13T23:33:00.000+0000</td><td>OTHER OFFENSES</td><td>TRAFFIC VIOLATION ARREST</td><td>Wednesday</td><td>NORTHERN</td><td>ARREST, BOOKED</td><td>VANNESS AV / GREENWICH ST</td><td>-122.42436302145</td><td>37.8004143219856</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, to_timestamp, date_format, col, year, desc, asc, hour, when, sum\n",
    "#Reading The data\n",
    "F = \"/FileStore/tables/Final P./train_*\"\n",
    "df = spark.read.csv(F,header=True,inferSchema=True)\n",
    "df = df.withColumn(\"Dates\",to_timestamp(df.Dates, format= \"MM/dd/yyyy HH:mm\"))\n",
    "display(df.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `` Categorizing String Data: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------------+--------+--------+---------+----------+----------+-------+-------------------+------------------+\n",
       "              Dates|Category|Descript|DayOfWeek|PdDistrict|Resolution|Address|                  X|                 Y|\n",
       "+-------------------+--------+--------+---------+----------+----------+-------+-------------------+------------------+\n",
       "2015-05-13 23:53:00|     7.0|     5.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|\n",
       "2015-05-13 23:53:00|     1.0|    44.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|\n",
       "2015-05-13 23:33:00|     1.0|    44.0|      1.0|       2.0|       1.0| 7781.0|   -122.42436302145|  37.8004143219856|\n",
       "2015-05-13 23:30:00|     0.0|     0.0|      1.0|       2.0|       0.0|  931.0|-122.42699532676599| 37.80087263276921|\n",
       "2015-05-13 23:30:00|     0.0|     0.0|      1.0|       8.0|       0.0| 3372.0|  -122.438737622757|37.771541172057795|\n",
       "+-------------------+--------+--------+---------+----------+----------+-------+-------------------+------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list(set(df.columns)-set(['Dates','X','Y'])) ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "xdf = pipeline.fit(df).transform(df)\n",
    "\n",
    "xdf = xdf.selectExpr('Dates','Category_index as Category','Descript_index as Descript','DayOfWeek_index as DayOfWeek','PdDistrict_index as\\\n",
    "                      PdDistrict','Resolution_index as Resolution','Address_index as Address','X','Y')\n",
    "xdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `` Creating Feature vector and Normalizing values: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "              Dates|label|Descript|DayOfWeek|PdDistrict|Resolution|Address|                  X|                 Y|            features|\n",
       "+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "2015-05-13 23:53:00|  7.0|     5.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|[0.0625,0.0012759...|\n",
       "2015-05-13 23:53:00|  1.0|    44.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|[0.0625,0.0012759...|\n",
       "2015-05-13 23:33:00|  1.0|    44.0|      1.0|       2.0|       1.0| 7781.0|   -122.42436302145|  37.8004143219856|[0.0625,0.0017695...|\n",
       "2015-05-13 23:30:00|  0.0|     0.0|      1.0|       2.0|       0.0|  931.0|-122.42699532676599| 37.80087263276921|[0.0,0.0017783484...|\n",
       "2015-05-13 23:30:00|  0.0|     0.0|      1.0|       8.0|       0.0| 3372.0|  -122.438737622757|37.771541172057795|[0.0,0.0012174329...|\n",
       "2015-05-13 23:30:00|  0.0|    53.0|      1.0|       6.0|       0.0| 2489.0|-122.40325236121201|   37.713430704116|[0.0,1.0616669583...|\n",
       "2015-05-13 23:30:00|  5.0|     3.0|      1.0|       6.0|       0.0|23142.0|  -122.423326976668|  37.7251380403778|[0.0,3.3005006570...|\n",
       "2015-05-13 23:30:00|  5.0|     3.0|      1.0|       3.0|       0.0|17481.0|  -122.371274317441|  37.7275640719518|[0.0,3.7644389193...|\n",
       "2015-05-13 23:00:00|  0.0|     0.0|      1.0|       9.0|       0.0| 8304.0|  -122.508194031117|37.776601260681204|[0.0,0.0013141987...|\n",
       "2015-05-13 23:00:00|  0.0|     0.0|      1.0|       4.0|       0.0| 6049.0|  -122.419087676747|  37.8078015516515|[0.0,0.0019108524...|\n",
       "+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "inputCols = list(set(xdf.columns)-set(['Dates','Category']))\n",
    "assembler = VectorAssembler().setInputCols(inputCols).setOutputCol(\"features\")\n",
    "transformVector = assembler.transform(xdf)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformVector)\n",
    "scaledData = scalerModel.transform(transformVector)\n",
    "scaledData = scaledData.drop('features').withColumnRenamed('scaledFeatures','features').withColumnRenamed('Category','label')\n",
    "train, test = scaledData.randomSplit([0.7, 0.3])\n",
    "scaledData.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#`  `` Classfiers: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#1`  `` logistic regression ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+--------------------+\n",
       "prediction|label|            features|\n",
       "+----------+-----+--------------------+\n",
       "       0.0|  0.0|[0.04499850832920...|\n",
       "       0.0|  7.0|[0.04624786179406...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  0.0|[0.02355800826683...|\n",
       "       2.0|  2.0|[0.02881654281117...|\n",
       "+----------+-----+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "<span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>0.25970100450926564\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#2`  `` Decision tree ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+--------------------+\n",
       "prediction|label|            features|\n",
       "+----------+-----+--------------------+\n",
       "       0.0|  0.0|[0.04499850832920...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  0.0|[0.02355800826683...|\n",
       "       1.0|  2.0|[0.02881654281117...|\n",
       "+----------+-----+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "<span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>0.38335221699225136\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#3`  `` Random forest ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+--------------------+\n",
       "prediction|label|            features|\n",
       "+----------+-----+--------------------+\n",
       "       0.0|  0.0|[0.04499850832920...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  0.0|[0.02355800826683...|\n",
       "       1.0|  2.0|[0.02881654281117...|\n",
       "+----------+-----+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "<span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>0.43263476468933215\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#4`  `` Naive Bayes ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+--------------------+\n",
       "prediction|label|            features|\n",
       "+----------+-----+--------------------+\n",
       "       0.0|  0.0|[0.04499850832920...|\n",
       "       0.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  0.0|[0.02355800826683...|\n",
       "       0.0|  2.0|[0.02881654281117...|\n",
       "+----------+-----+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "<span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>0.20225539324902858\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "nbModel = nb.fit(train)\n",
    "predictions = nbModel.transform(test)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        `#5`  `` One-vs-Rest (LogisticRegression) ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+-----+--------------------+\n",
       "prediction|label|            features|\n",
       "+----------+-----+--------------------+\n",
       "       0.0|  0.0|[0.04499850832920...|\n",
       "       0.0|  7.0|[0.04624786179406...|\n",
       "       1.0|  7.0|[0.04624786179406...|\n",
       "       0.0|  0.0|[0.02355800826683...|\n",
       "       2.0|  2.0|[0.02881654281117...|\n",
       "+----------+-----+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "<span class=\"ansired\">Out[</span><span class=\"ansired\">10</span><span class=\"ansired\">]: </span>0.25905845316218906\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "ovr = OneVsRest( classifier=lr )\n",
    "\n",
    "ovrModel = ovr.fit(train)\n",
    "predictions = ovrModel.transform(test)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `` Paramter Tuning (Best 3 Classfiers): ``\n",
    "##### `` #1 logistic regression: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">34</span><span class=\"ansired\">]: </span>0.2540196350662046\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.regParam,[0.01, 0.5, 2.0])\\\n",
    "    .addGrid(lr.tol, [1e-04, 1e-06, 1e-08])\\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator())\n",
    "\n",
    "lrModel = crossval.fit(train)\n",
    "predictions = lrModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "bestPipeline = lrModel.bestModel\n",
    "bestLRModel = bestPipeline.stages[0]\n",
    "bestParams = bestLRModel.extractParamMap()\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">35</span><span class=\"ansired\">]: </span>\n",
       "{Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;aggregationDepth&apos;, doc=&apos;suggested depth for treeAggregate (&gt;= 2)&apos;): 2,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;elasticNetParam&apos;, doc=&apos;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty&apos;): 0.0,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;family&apos;, doc=&apos;The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.&apos;): &apos;auto&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;probabilityCol&apos;, doc=&apos;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&apos;): &apos;probability&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;threshold&apos;, doc=&apos;threshold in binary classification prediction, in range [0, 1]&apos;): 0.5,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;rawPredictionCol&apos;, doc=&apos;raw prediction (a.k.a. confidence) column name&apos;): &apos;rawPrediction&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;labelCol&apos;, doc=&apos;label column name&apos;): &apos;label&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;regParam&apos;, doc=&apos;regularization parameter (&gt;= 0)&apos;): 0.01,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;maxIter&apos;, doc=&apos;maximum number of iterations (&gt;= 0)&apos;): 100,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;featuresCol&apos;, doc=&apos;features column name&apos;): &apos;features&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;standardization&apos;, doc=&apos;whether to standardize the training features before fitting the model&apos;): True,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;predictionCol&apos;, doc=&apos;prediction column name&apos;): &apos;prediction&apos;,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;tol&apos;, doc=&apos;the convergence tolerance for iterative algorithms (&gt;= 0)&apos;): 1e-06,\n",
       " Param(parent=&apos;LogisticRegression_bf178aad5b63&apos;, name=&apos;fitIntercept&apos;, doc=&apos;whether to fit an intercept term&apos;): True}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LR Best Paramters are: [``elasticNetParam:0.0``, ``regParam:0.01``, ``tol:1e-06``]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `` #2 Decision tree: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>0.7522980696973607\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxMemoryInMB,[256, 512,1024])\\\n",
    "    .addGrid(dt.checkpointInterval,[5, 10,20])\\\n",
    "    .addGrid(dt.maxBins,[32, 64,128])\\\n",
    "    .addGrid(dt.maxDepth,[5, 10,15])\\\n",
    "    .addGrid(dt.minInstancesPerNode,[2, 4,10])\\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator())\n",
    "\n",
    "dtModel = crossval.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "bestPipeline = dtModel.bestModel\n",
    "bestDTModel = bestPipeline.stages[0]\n",
    "bestParams = bestDTModel.extractParamMap()\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">25</span><span class=\"ansired\">]: </span>\n",
       "{Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;maxMemoryInMB&apos;, doc=&apos;Maximum memory in MB allocated to histogram aggregation.&apos;): 256,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;checkpointInterval&apos;, doc=&apos;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&apos;): 10,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;rawPredictionCol&apos;, doc=&apos;raw prediction (a.k.a. confidence) column name&apos;): &apos;rawPrediction&apos;,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;maxBins&apos;, doc=&apos;Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature.&apos;): 32,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;minInfoGain&apos;, doc=&apos;Minimum information gain for a split to be considered at a tree node.&apos;): 0.0,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;maxDepth&apos;, doc=&apos;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&apos;): 5,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;featuresCol&apos;, doc=&apos;features column name&apos;): &apos;features&apos;,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;impurity&apos;, doc=&apos;Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini&apos;): &apos;gini&apos;,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;predictionCol&apos;, doc=&apos;prediction column name&apos;): &apos;prediction&apos;,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;minInstancesPerNode&apos;, doc=&apos;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1.&apos;): 1,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;seed&apos;, doc=&apos;random seed&apos;): 3496922717701084861,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;labelCol&apos;, doc=&apos;label column name&apos;): &apos;label&apos;,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;cacheNodeIds&apos;, doc=&apos;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&apos;): True,\n",
       " Param(parent=&apos;DecisionTreeClassifier_94bdf0c11a28&apos;, name=&apos;probabilityCol&apos;, doc=&apos;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&apos;): &apos;probability&apos;}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DT Best Paramters are: [``maxMemoryInMB:256``, ``checkpointInterval:10``, ``maxBins:32``, ``maxDepth:5``, ``minInstancesPerNode:1``]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `` #3 Random Forest: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">36</span><span class=\"ansired\">]: </span>0.5767353754512223\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees,[10, 25, 50])\\\n",
    "    .addGrid(rf.maxDepth,[4, 8, 10])\\\n",
    "    .addGrid(rf.maxBins,[8, 16, 32]) \\\n",
    "    .build()\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator())\n",
    "\n",
    "rfModel = crossval.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "bestPipeline = rfModel.bestModel\n",
    "bestRFModel = bestPipeline.stages[0]\n",
    "bestParams = bestRFModel.extractParamMap()\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">37</span><span class=\"ansired\">]: </span>\n",
       "{Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;cacheNodeIds&apos;, doc=&apos;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&apos;): False,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;maxBins&apos;, doc=&apos;Max number of bins for discretizing continuous features.  Must be &gt;=2 and &gt;= number of categories for any categorical feature.&apos;): 32,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;checkpointInterval&apos;, doc=&apos;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&apos;): 10,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;probabilityCol&apos;, doc=&apos;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&apos;): &apos;probability&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;maxDepth&apos;, doc=&apos;Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&apos;): 10,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;rawPredictionCol&apos;, doc=&apos;raw prediction (a.k.a. confidence) column name&apos;): &apos;rawPrediction&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;featuresCol&apos;, doc=&apos;features column name&apos;): &apos;features&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;minInstancesPerNode&apos;, doc=&apos;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1.&apos;): 1,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;seed&apos;, doc=&apos;random seed&apos;): -4110891593024638465,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;featureSubsetStrategy&apos;, doc=&apos;The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].&apos;): &apos;auto&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;minInfoGain&apos;, doc=&apos;Minimum information gain for a split to be considered at a tree node.&apos;): 0.0,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;maxMemoryInMB&apos;, doc=&apos;Maximum memory in MB allocated to histogram aggregation.&apos;): 256,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;impurity&apos;, doc=&apos;Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini&apos;): &apos;gini&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;predictionCol&apos;, doc=&apos;prediction column name&apos;): &apos;prediction&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;labelCol&apos;, doc=&apos;label column name&apos;): &apos;label&apos;,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;numTrees&apos;, doc=&apos;Number of trees to train (&gt;= 1)&apos;): 50,\n",
       " Param(parent=&apos;RandomForestClassifier_1594dc221845&apos;, name=&apos;subsamplingRate&apos;, doc=&apos;Fraction of the training data used for learning each decision tree, in range (0, 1].&apos;): 1.0}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RF Best Paramters are: [``numTrees:50``, ``maxDepth:10``, ``maxBins:32``]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `` Selecting Colums that match the (Test Data) and Transforming it: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "              Dates|label|Descript|DayOfWeek|PdDistrict|Resolution|Address|                  X|                 Y|            features|\n",
       "+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "2015-05-13 23:53:00|  7.0|     5.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|[0.04357794799292...|\n",
       "2015-05-13 23:53:00|  1.0|    44.0|      1.0|       2.0|       1.0| 3991.0|  -122.425891675136|  37.7745985956747|[0.04357794799292...|\n",
       "2015-05-13 23:33:00|  1.0|    44.0|      1.0|       2.0|       1.0| 7781.0|   -122.42436302145|  37.8004143219856|[0.04433709665847...|\n",
       "2015-05-13 23:30:00|  0.0|     0.0|      1.0|       2.0|       0.0|  931.0|-122.42699532676599| 37.80087263276921|[0.04302986069898...|\n",
       "2015-05-13 23:30:00|  0.0|     0.0|      1.0|       8.0|       0.0| 3372.0|  -122.438737622757|37.771541172057795|[0.03719848867946...|\n",
       "+-------------------+-----+--------+---------+----------+----------+-------+-------------------+------------------+--------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Selecting Colums that match the (Test Data)\n",
    "xxdf = xdf.selectExpr('Dates','Category','Descript','DayOfWeek','PdDistrict','Resolution','Address','X','Y')\n",
    "inputCols = list(set(xdf.columns)-set(['Dates','Category','Descript','Resolution']))\n",
    "#Feature Vector\n",
    "assembler = VectorAssembler().setInputCols(inputCols).setOutputCol(\"features\")\n",
    "transformVector = assembler.transform(xxdf)\n",
    "#Normalizing\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformVector)\n",
    "scaledData = scalerModel.transform(transformVector)\n",
    "train = scaledData.drop('features').withColumnRenamed('scaledFeatures','features').withColumnRenamed('Category','label')\n",
    "train.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `` Test Data and it's Transformations: ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Dates</th><th>DayOfWeek</th><th>PdDistrict</th><th>Address</th><th>X</th><th>Y</th></tr></thead><tbody><tr><td>0</td><td>2015-05-10T23:59:00.000+0000</td><td>Sunday</td><td>BAYVIEW</td><td>2000 Block of THOMAS AV</td><td>-122.39958770418998</td><td>37.7350510103906</td></tr><tr><td>1</td><td>2015-05-10T23:51:00.000+0000</td><td>Sunday</td><td>BAYVIEW</td><td>3RD ST / REVERE AV</td><td>-122.391522893042</td><td>37.7324323864471</td></tr><tr><td>2</td><td>2015-05-10T23:50:00.000+0000</td><td>Sunday</td><td>NORTHERN</td><td>2000 Block of GOUGH ST</td><td>-122.426001954961</td><td>37.7922124386284</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reading The Test data\n",
    "Ft = \"/FileStore/tables/Final P./test_*\"\n",
    "dft = spark.read.csv(Ft,header=True,inferSchema=True)\n",
    "dft = dft.withColumn(\"Dates\",to_timestamp(dft.Dates, format= \"MM/dd/yyyy HH:mm\"))\n",
    "display(dft.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+-------------------+---------+----------+-------+-------------------+----------------+--------------------+\n",
       " Id|              Dates|DayOfWeek|PdDistrict|Address|                  X|               Y|            features|\n",
       "+---+-------------------+---------+----------+-------+-------------------+----------------+--------------------+\n",
       "  0|2015-05-10 23:59:00|      6.0|       3.0| 2638.0|-122.39958770418998|37.7350510103906|[0.33333333333333...|\n",
       "  1|2015-05-10 23:51:00|      6.0|       3.0|  675.0|  -122.391522893042|37.7324323864471|[0.33333333333333...|\n",
       "  2|2015-05-10 23:50:00|      6.0|       2.0| 8645.0|  -122.426001954961|37.7922124386284|[0.22222222222222...|\n",
       "  3|2015-05-10 23:45:00|      6.0|       6.0|  219.0|  -122.437393972517|37.7214120621391|[0.66666666666666...|\n",
       "  4|2015-05-10 23:45:00|      6.0|       6.0|  219.0|  -122.437393972517|37.7214120621391|[0.66666666666666...|\n",
       "+---+-------------------+---------+----------+-------+-------------------+----------------+--------------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dft) for column in list(set(dft.columns)-set(['Dates','X','Y'])) ]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "xdft = pipeline.fit(dft).transform(dft)\n",
    "\n",
    "xdft = xdft.selectExpr('Id','Dates','DayOfWeek_index as DayOfWeek','PdDistrict_index as PdDistrict','Address_index as Address','X','Y')\n",
    "\n",
    "inputCols = list(set(xdft.columns)-set(['Id','Dates']))\n",
    "assembler = VectorAssembler().setInputCols(inputCols).setOutputCol(\"features\")\n",
    "transformVector = assembler.transform(xdft)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(transformVector)\n",
    "scaledData = scalerModel.transform(transformVector)\n",
    "test = scaledData.drop('features').withColumnRenamed('scaledFeatures','features')\n",
    "test.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `` Testing : ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(maxMemoryInMB=256, checkpointInterval=5, maxBins=64, maxDepth=15, minInstancesPerNode=2)\n",
    "\n",
    "Model = dt.fit(train)\n",
    "predictions = Model.transform(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `` Return String labels : ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+---+--------------+--------------------+--------------------+\n",
       " Id|         label|         probability|            features|\n",
       "+---+--------------+--------------------+--------------------+\n",
       "  0|  NON-CRIMINAL|[0.0,0.0,0.5,0.0,...|[0.33333333333333...|\n",
       "  1|  NON-CRIMINAL|[0.0,0.0,0.5,0.0,...|[0.33333333333333...|\n",
       "  2| LARCENY/THEFT|[0.33333333333333...|[0.22222222222222...|\n",
       "  3|OTHER OFFENSES|[0.05970149253731...|[0.66666666666666...|\n",
       "  4|OTHER OFFENSES|[0.05970149253731...|[0.66666666666666...|\n",
       "+---+--------------+--------------------+--------------------+\n",
       "only showing top 5 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "#Getting the List of 'labels' (String)\n",
    "xindexers = StringIndexer(inputCol='Category', outputCol='label').fit(df)\n",
    "xxInt = xindexers.transform(df)\n",
    "Ze = xxInt.select('Category','label').groupBy('Category','label').count().orderBy(asc(\"label\"))\n",
    "Zee = list(Ze.select('Category').toPandas()['Category'])\n",
    "#Mapping each StringLabel with IntLabel (returns Strings)\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"label\", labels=Zee)\n",
    "converted = converter.transform(predictions)\n",
    "converted.select('Id',\"label\", \"probability\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(converted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Final P. (ML)",
  "notebookId": 3042052203603541
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
